# ⚙️ Physical AI is having its ChatGPT moment

## Email Details

- **From:** The Deep View <newsletter@thedeepview.co>
- **To:** "petere@roksys.co.uk" <petere@roksys.co.uk>
- **Date:** 2025-12-14 13:36:46
- **Gmail ID:** 19b1d13ff7b132a7

---

## Message

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/b7d01607-cdd3-480f-89d1-53ba3f117fcc/BANNER__5_.jpg?t=1765692306)
Caption: 

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/8cdabcf6-65d6-4643-93df-96f0c97954d6/Together_W_You.com.png?t=1765486357)
Follow image link: (https://about.you.com/ai-training-checklist?utm_campaign=31224137-The_Deep_View_Q4-Q1&utm_source=external-newsletter&utm_medium=email&utm_term=deepview_triplehit_1214&utm_content=deepview_triplehit_1214)
Caption: 

**Welcome Back. **The robots are taking over. Or, at least, they’ll be able to fold our laundry and do our dishes once they learn how to move their fingers properly. The physical AI market is rapidly accelerating as developers and researchers seek new ways to make AI that sees the world as humans do. But as it turns out, making a robot that actually works isn’t an easy feat.

**IN TODAY’S NEWSLETTER**

## 1. Physical AI goes from words to worlds

## 2. Robots have a data problem 

## 3. Why AGI might need a body

----------
RESEARCH

# Physical AI goes from words to worlds

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/f92596ce-17cb-40f4-b622-c02e586d8f32/ART1__3_.jpg?t=1765692310)
Caption: 

Physical AI might be on the verge of a ChatGPT moment. 

Robotics, vision models that turn language into action, and the buzzy, emerging concept of world models have caught the attention of Big Tech, startups and investors alike. Though developing these models comes with a unique set of challenges, physical AI is in a “GPT 1 or 2 phase,” Evan Helda, head of physical AI at Nebius, told The Deep View. 

“You have the early signs of a general purpose model, which is like the ChatGPT moment,” Helda told me at the Nebius Robotics and Physical AI Summit on Tuesday. “We’re in that 2018, 2019 period for LLMs. We’re not there yet, but people aren't as caught off guard now. People are trying to get ahead, versus playing catch up.” 

According to a report by Crunchbase and tech nonprofit Mind the Bridge released in September, [physical AI firms raked in $16.1 billion](https://mindthebridge.com/silicon-valley-bets-big-on-physical-ai-93-of-vc-flows-now-go-into-artificial-intelligence/) in the first three quarters of 2025. Some firms have seen standout investments since then, including [Figure AI’s $1 billion Series C](https://www.figure.ai/news/series-c) at a valuation of $39 billion; Physical Intelligence’s $600 million round, [propelling it to a $5.6 billion valuation](https://www.humanoidsdaily.com/news/physical-intelligence-secures-600-million-to-build-a-universal-robot-brain-hitting-5-6-billion-valuation); and Jeff Bezos’ Project Prometheus, which [emerged from stealth with $6.2 billion in funding](https://www.nytimes.com/2025/11/17/technology/bezos-project-prometheus.html). 

Beyond the numbers, physical AI deployments run the gamut from [medical deliveries](https://ottonomy.io/) to [helicopter piloting during wildfires](https://www.voxelis.ai/) to [weeding fields](https://www.aigen.io/). Some of AI’s most prominent researchers have shifted their focus from [language models to world models](https://archive.thedeepview.com/p/eu-launches-ai-plan-to-catch-us-and-china), capable of comprehending the world as humans see it.

It’s proof that AI is moving far beyond the “digital domain,” Amit Goel, head of robotics and edge computing ecosystem at Nvidia, said in his keynote on Tuesday. 

Constructing these systems is no easy feat. Researchers are starting to create models that understand modalities beyond language and vision. But vectors like space, time, force and touch are all concepts that a physical AI system needs to grasp, said Goel. 

“Inherently, physical AI has to be multimodal,” Goel said. “We have to understand all the different modalities in order to make intelligent decisions.”

However, Nebius’ San Jose summit showcased startups working on physical AI capabilities across the stack, from simulation to foundation models to visual intelligence to actual robotic deployment. With researchers undertaking these problems at all angles, the industry is in for “a lot of progress in parallel,” said Brad Porter, co-founder and CEO of Cobot. 

“There are enough people now working on it that I think we’re going to see really big breakthroughs in the next few years,” Porter said in a fireside chat on Tuesday. “It just might not be through brute force scaling like LLMs had.”


--------------------
TOGETHER WITH [YOU.COM](https://about.you.com/ai-training-checklist?utm_campaign=31224137-The_Deep_View_Q4-Q1&utm_source=external-newsletter&utm_medium=email&utm_term=deepview_triplehit_1214&utm_content=deepview_triplehit_1214)

# [When training takes a backseat, your AI programs don't stand a chance.](https://about.you.com/ai-training-checklist?utm_campaign=31224137-The_Deep_View_Q4-Q1&utm_source=external-newsletter&utm_medium=email&utm_term=deepview_triplehit_1214&utm_content=deepview_triplehit_1214)

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/f6259542-0de4-4a12-a001-741bb37167f1/The_AI_Training_Checklist_Graphic_Deepview_600x400.png?t=1765486560)
Follow image link: (https://about.you.com/ai-training-checklist?utm_campaign=31224137-The_Deep_View_Q4-Q1&utm_source=external-newsletter&utm_medium=email&utm_term=deepview_triplehit_1214&utm_content=deepview_triplehit_1214)
Caption: 

Lack of training is one of the biggest reasons AI adoption stalls. This [AI Training Checklist from You.com](https://about.you.com/ai-training-checklist?utm_campaign=31224137-The_Deep_View_Q4-Q1&utm_source=external-newsletter&utm_medium=email&utm_term=deepview_triplehit_1214&utm_content=deepview_triplehit_1214) highlights common pitfalls to avoid and necessary steps to take when building a capable, confident team that can make the most out of your AI investment. 

What you'll get:

* Key steps for building a successful AI training program

* Guidance on fostering adoption

* A structured worksheet to monitor progress across your organization

Set your AI initiatives on the right track. [Get the checklist.](https://about.you.com/ai-training-checklist?utm_campaign=31224137-The_Deep_View_Q4-Q1&utm_source=external-newsletter&utm_medium=email&utm_term=deepview_triplehit_1214&utm_content=deepview_triplehit_1214)


--------------------
PRODUCTS

# Robots have a data problem 

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/b9593f09-4179-47c2-a69d-a8f80cb12f62/ART2__2_.jpg?t=1765692307)
Caption: 

Though tech firms have lofty aspirations for a robotic future where general-purpose humanoids walk among us, actually making physical AI systems that work is far from simple. 

From the [foundation that these machines](https://palatial.cloud/) are built on all the way down to the [dexterity of robotic fingers themselves](https://www.rlwrld.ai/), building physical intelligence systems comes with several sets of problems that large language model development doesn’t face. To accurately reflect and take action on the world around them, these models need to comprehend their surroundings in a way that a chatbot can’t. 

“Building a robot is not easy. I often say it’s like raising a kid. You need a village to do that,” Nvidia’s Goel said in his Tuesday keynote. 

One of the foundational rules of AI is that a model is only as good as the data that it’s trained on. But that is squarely the issue with a physical model: Getting good data involves more than just scraping the internet. For the models to go beyond words, the data needs to go beyond words, too. 

It’s a problem, Ken Goldberg, a UC Berkeley roboticist, dubbed the [100,000-year data gap:](https://www.science.org/doi/10.1126/scirobotics.aea7390) the amount of training data needed for general-purpose humanoid robotics vastly outweighs the training needs of modern LLMs. “There's not enough real world data out there,” Nebius’ Helda told The Deep View. 

* While it’s possible for humans to capture real-world data, Helda noted, it only scales with a human in the loop. 

* And though deploying robots in the real world can generate a wealth of data, that data only goes as far as the robots that can be deployed, TJ Galda, senior director of product management for Nvidia’s COSMOS world models, told The Deep View. “If you build a brand new robot and you've never deployed it, you've got zero video data, zero LIDAR.” 

* Additionally, even if a robot or physical AI system only needs to be trained on a specific task, these models still require a vast amount of pretraining data to scale, Lindon Gao, CEO of Dyna Robotics, said during a panel at the Nebius event. “We need to scale pretraining data sets much more drastically so we can cover a wider distribution of data across different types of tasks.” 

It’s why synthetic data, or data that mimics the conditions of the real world, generated in world models might be key to closing this gap, said Galda. In addition to generating large datasets to train self-driving cars and other autonomous systems, these systems can simulate rare conditions to better prepare models for anomalies, he said.

For example, while a dash cam might capture a video of a bear running across the road once, a simulation in a world model could replicate this action hundreds or thousands of times, he said. 

“Can we build synthetic data that's good enough to train on that's just as good as if you hit record on a camera? Because if we can, then we can scale that problem very quickly,” said Galda.


--------------------
TOGETHER WITH [YOU.COM](https://about.you.com/ai-training-checklist?utm_campaign=31224137-The_Deep_View_Q4-Q1&utm_source=external-newsletter&utm_medium=email&utm_term=deepview_triplehit_1214&utm_content=deepview_triplehit_1214)

# [Start 2026 on the right foot by giving your team the AI training they need.](https://about.you.com/ai-training-checklist?utm_campaign=31224137-The_Deep_View_Q4-Q1&utm_source=external-newsletter&utm_medium=email&utm_term=deepview_triplehit_1214&utm_content=deepview_triplehit_1214)

AI implementation can go sideways due to unclear goals and lack of skills. Ensure your team is ready to harness the full potential of your AI investment with this[ AI Training Checklist from You.com](https://about.you.com/ai-training-checklist?utm_campaign=31224137-The_Deep_View_Q4-Q1&utm_source=external-newsletter&utm_medium=email&utm_term=deepview_triplehit_1214&utm_content=deepview_triplehit_1214). Set your team—and your AI initiatives—up for success in the new year.

[Get the checklist and set yourself up for success](https://about.you.com/ai-training-checklist?utm_campaign=31224137-The_Deep_View_Q4-Q1&utm_source=external-newsletter&utm_medium=email&utm_term=deepview_triplehit_1214&utm_content=deepview_triplehit_1214).


--------------------
HARDWARE

# Why AGI might need a body

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/8012ca2a-24b1-4daa-a5ba-04e98d831e3f/ART3__1_.jpg?t=1765692312)
Caption: 

While a lot of money, time and effort have been poured into making large language models bigger and better, machines that are good with words are just one ingredient in a larger recipe. 

LLMs play an important “low-level function” in physical AI and robotics, Helda told The Deep View. Words are how humans can functionally communicate with these systems, and language models can translate those words into action. But human understanding spans far beyond language, he noted.

Major AI researchers like [Yann LeCun](https://www.youtube.com/watch?v=4__gg83s_Do&utm_source=thedeepview&utm_medium=newsletter&utm_campaign=llms-won-t-get-us-to-agi-sustkever), [Fei-Fei Li](https://archive.thedeepview.com/p/eu-could-loosen-privacy-tech-regulations) and [Ilya Sutskever](https://archive.thedeepview.com/p/llms-won-t-get-us-to-agi-sustkever) have questioned the idea that scaling large language models will lead to the idealistic vision of artificial general intelligence, and some turned their attention to world models and spatial intelligence as the next frontier. 

“How do you really have agency in the world and be truly intelligent – know what you're doing and why you're doing it – unless you have a sense of consequence? A sense of physics?”** **said Helda. “Until you can understand physics and movement and cause and effect, I don't think you can be truly super intelligent. And LLMs don't have that.”

Giving these machines an embodiment might help them exist beyond just being a [“brain in a can,”](https://archive.thedeepview.com/p/eu-launches-ai-plan-to-catch-us-and-china) as RLWRLD CEO Jung-hee Ryu told me last week at the AWS re:Invent conference in Las Vegas. “If we decide to build artificial general intelligence or artificial super intelligence, we should give it a body, an embodiment, to feel real-world situations and sensory information.”

However, while the humanoid form factor comes with the benefit of versatility, “The technology is not there yet. It’s coming, but it’s not there,” Jonathan Hurst, CEO of Agility AI, said last week on a panel at the Nebius event. “It’s going to be a while before you have this generally capable humanoid that can operate in all of those spaces.”  

While large language models are always going to be a piece of the puzzle, the question remains of whether or not the industry has put too much faith in these models’ capabilities. Rumblings of an AI bubble have emerged in recent months as LLM developers pour upwards of a trillion dollars into AI infrastructure with little returns to show for it. The largely nascent market for physical intelligence will also be able to leverage this infrastructure. As Hugging Face CEO Clem Delangue put it, we may be in an [LLM bubble](https://techcrunch.com/2025/11/18/hugging-face-ceo-says-were-in-an-llm-bubble-not-an-ai-bubble/), rather than an AI bubble broadly. 

“You can keep getting better and better LLMs, perhaps, but certainly it's reliant on them being used for something,” Thomas Randall, research director at Info-Tech Research Group, told The Deep View. “There's this race of innovation, but that's now way ahead of how far users can keep up with it.”


--------------------
LINKS

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/abfc58a8-a261-43c3-b257-3ccb82d195f0/image.png?t=1752986903)
Caption: 

* Video communication firm [Zoom is now a frontier AI company](https://x.com/zoom/status/1999159317103292610?s=46&t=-0uT4XoIJ-cjXoQMtjEyVg)

* Broadcom reveals [Anthropic is its mystery $10 billion customer](https://www.cnbc.com/2025/12/11/broadcom-reveals-its-mystery-10-billion-customer-is-anthropic.html)

* ChatGPT [“adult mode” to roll out early next year](https://www.theverge.com/news/842657/openai-chatgpt-adult-mode-debut-q1-2026), once age prediction is ready

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/59d274bf-9e99-4acf-a6c7-ff9cdbb0233e/image.png?t=1752986916)
Caption: 

* [Tinker](https://thinkingmachines.ai/blog/tinker-general-availability/): Thinking Machines has made Tinker, its language model API, generally available. 

* [Gemini Audio Updates:](https://x.com/GoogleAI/status/1999560839679082507?s=20) Google has added speech-to-speech translation, improved text-to-speech capabilities and complex workflow improvements to its flagship models.

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/25296119-9f00-4620-9931-2d98d0068485/image.png?t=1752986966)
Caption: 

* [Amazon](https://www.amazon.jobs/en/jobs/3139033/applied-scientist-agi?cmpid=SPLICX0248M): Applied Scientist, AGI

* [xAI](https://job-boards.greenhouse.io/xai/jobs/4949014007?gh_src=b4acz7ed7us): Member of Technical Staff - Multimodal Interactions Post-training


--------------------
GAMES

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/d120b54c-d56c-4fbc-bdbf-41e83cbbc770/image.png?t=1752987116)
Caption: 

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/65497fd1-50cc-402e-9fe9-31142dc3cdee/AI_or_Not_-_Portrait_-_072025__21_.png?t=1765693728)
Caption: 


--------------------
A POLL BEFORE YOU GO


----------The Deep View is written by Nat Rubio-Licht, Jack Kubinec, Jason Hiner, Faris Kojok and The Deep View crew. Please reply with any feedback!

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/6348eb1c-4686-4f97-b329-5ada23432426/Frame_1410104611.png?t=1765259325)
Caption: 

Thanks for reading today’s edition of The Deep View! We’ll see you in the next one.

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/f13ecf40-963d-4c4e-8059-8f41083085a8/Frame_1410104595.png?t=1765345242)
Caption: 

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/77392f54-56e4-430e-b7a3-70ccc70b6bbb/Real.png?t=1765692939)
Caption: Link to the original

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/d9da2bf5-4da3-4a9e-bd33-c508d0439661/Fake.png?t=1765692948)
Caption: Link to the AI image

*Editor’s note: Turns out we were the ones who couldn't tell Fake from Real last time. Corrected version above!

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/f6b2537a-c207-453e-8567-5898374096f2/Frame_1410104596.png?t=1765345268)
Caption: 

Take The Deep View with you on the go! We’ve got exclusive, in-depth interviews for you on The Deep View: Conversations podcast every Tuesday morning.

SUBSCRIBE (https://tdv.transistor.fm/)

View image: (https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/a09d75bf-0585-4881-8e41-db84e8df85d0/Frame_1410104597.png?t=1765345268)
Caption: 

If you want to get in front of an audience of 450,000+ developers, business leaders and tech enthusiasts, get in touch with us here.

GET IN TOUCH WITH US HERE (https://www.passionfroot.me/the-deep-view)


———

You are reading a plain text version of this post. For the best experience, copy and paste this link in your browser to view the post online:
https://archive.thedeepview.com/p/physical-ai-goes-from-words-to-worlds

